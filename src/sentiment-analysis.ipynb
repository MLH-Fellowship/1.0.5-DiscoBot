{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import torchtext.experimental\n",
    "import torchtext.experimental.vectors\n",
    "\n",
    "import collections\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import ep_time, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.experimental.datasets.raw.text_classification import \\\n",
    "    RawTextIterableDataset\n",
    "from torchtext.experimental.datasets.text_classification import \\\n",
    "    TextClassificationDataset\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "\n",
    "def get_train_valid_split(raw_train, split_ratio=0.7):\n",
    "    raw_train = list(raw_train)\n",
    "    random.shuffle(raw_train)\n",
    "\n",
    "    n_train_ex = int(len(raw_train) * split_ratio)\n",
    "    train_data = raw_train[:n_train_ex]\n",
    "    valid_data = raw_train[n_train_ex:]\n",
    "    return train_data, valid_data\n",
    "\n",
    "\n",
    "def gen_vocab(raw_data, tokenizer, **vocab_kwargs):\n",
    "    token_freqs = collections.Counter()\n",
    "\n",
    "    for label, text in raw_data:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_freqs.update(tokens)\n",
    "\n",
    "    vocab = torchtext.vocab.Vocab(token_freqs, **vocab_kwargs)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def process_raw(raw_data, tokenizer, vocab):\n",
    "    raw_data = [(label, text) for (label, text) in raw_data]\n",
    "    text_trans = sequential_transforms(\n",
    "        tokenizer.tokenize, vocab_func(vocab), to_tensor(dtype=torch.long)\n",
    "    )\n",
    "    label_trans = sequential_transforms(to_tensor(dtype=torch.long))\n",
    "\n",
    "    transforms = (label_trans, text_trans)\n",
    "\n",
    "    return TextClassificationDataset(raw_data, vocab, transforms)\n",
    "\n",
    "\n",
    "def init_params(m: nn.Module):\n",
    "    if isinstance(m, nn.Embedding):\n",
    "        nn.init.uniform_(m.weight, -0.05, 0.05)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for n, p in m.named_parameters():\n",
    "            if \"weight_ih\" in n:\n",
    "                i, f, g, o = p.chunk(4)\n",
    "                nn.init.xavier_uniform_(i)\n",
    "                nn.init.xavier_uniform_(f)\n",
    "                nn.init.xavier_uniform_(g)\n",
    "                nn.init.xavier_uniform_(o)\n",
    "            elif \"weight_hh\" in n:\n",
    "                i, f, g, o = p.chunk(4)\n",
    "                nn.init.orthogonal_(i)\n",
    "                nn.init.orthogonal_(f)\n",
    "                nn.init.orthogonal_(g)\n",
    "                nn.init.orthogonal_(o)\n",
    "            elif \"bias\" in n:\n",
    "                i, f, g, o = p.chunk(4)\n",
    "                nn.init.zeros_(i)\n",
    "                nn.init.ones_(f)\n",
    "                nn.init.zeros_(g)\n",
    "                nn.init.zeros_(o)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def get_pretrained_embedding(init_embed, pretrained_vectors, vocab, unk_token):\n",
    "    pretrained_embedding = torch.FloatTensor(init_embed.weight.clone()).detach()\n",
    "    # pretrained_vocab = pretrained_vectors.vectors.get_stoi()\n",
    "\n",
    "    unk_tokens = []\n",
    "\n",
    "    for idx, token in enumerate(vocab.itos):\n",
    "        # if token in pretrained_vocab:\n",
    "        pretrained_vector = pretrained_vectors[token]\n",
    "        pretrained_embedding[idx] = pretrained_vector\n",
    "        # else:\n",
    "            # unk_tokens.append(token)\n",
    "\n",
    "    return pretrained_embedding, unk_tokens\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    ep_loss, ep_acc = 0, 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for labels, text, lengths in iterator:\n",
    "        labels, text = labels.to(device), text.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text, lengths)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "\n",
    "        acc = calc_acc(predictions, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ep_loss += loss.item()\n",
    "        ep_acc += acc.item()\n",
    "\n",
    "    return ep_loss / len(iterator), ep_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    ep_loss, ep_acc = 0, 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for labels, text, lengths in iterator:\n",
    "            labels, text = labels.to(device), text.to(device)\n",
    "\n",
    "            predictions = model(text, lengths)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            acc = calc_acc(predictions, labels)\n",
    "\n",
    "            ep_loss += loss.item()\n",
    "            ep_acc += acc.item()\n",
    "\n",
    "    return ep_loss / len(iterator), ep_acc / len(iterator)\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, fn=\"basic_english\", lower=True, max_len=None):\n",
    "        self.tokenize_fn = torchtext.data.utils.get_tokenizer(fn)\n",
    "        self.lower = lower\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        tokens = self.tokenize_fn(s)\n",
    "\n",
    "        if self.lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "\n",
    "        if self.max_len is not None:\n",
    "            tokens = tokens[: self.max_len]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class Collator:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def collate(self, batch):\n",
    "        labels, text = zip(*batch)\n",
    "        labels, lengths = torch.LongTensor(labels), torch.LongTensor(\n",
    "            [len(x) for x in text]\n",
    "        )\n",
    "\n",
    "        text = nn.utils.rnn.pad_sequence(text, padding_value=self.pad_idx)\n",
    "        return labels, text, lengths\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, emb_dim, hid_dim, output_dim, n_layer, dropout, pad_idx\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hid_dim, num_layers=n_layer, bidirectional=True, dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(2 * hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        # [seq_len, batch_size, emb_dim]\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120\n",
    "        packed_emb = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, (hidden, cell) = self.lstm(packed_emb)\n",
    "\n",
    "        # outputs : [seq_len, batch_size, n_direction * hid_dim]\n",
    "        # hid : [n_layers * n_direction, batch_size, hid_dim]\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out)\n",
    "\n",
    "        # [batch_size, hid_dim]\n",
    "        hidden_fwd, hidden_bck = hidden[-2], hidden[-1]\n",
    "        # [batch_size, hid_dim*2]\n",
    "        hidden = torch.cat((hidden_fwd, hidden_bck), dim=1)\n",
    "        # pred : [batch_size, output_dim]\n",
    "        return self.fc(self.dropout(hidden))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "max_len = 500\n",
    "max_size = 25000\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train, raw_test = torchtext.experimental.datasets.raw.IMDB()\n",
    "raw_train, raw_valid = get_train_valid_split(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = gen_vocab(raw_train, tokenizer, max_size=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_raw(raw_train,tokenizer, vocab)\n",
    "test_data = process_raw(raw_test,tokenizer, vocab)\n",
    "valid_data = process_raw(raw_valid,tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "pad_token = '<pad>'\n",
    "unk_token = '<unk>'\n",
    "pad_idx = vocab[pad_token]\n",
    "input_dim = len(vocab)\n",
    "emb_dim = 100\n",
    "hid_dim = 256\n",
    "output_dim = 2\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "n_epochs = 10\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator(pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True, collate_fn=collator.collate)\n",
    "\n",
    "valid_iterator = torch.utils.data.DataLoader(valid_data, batch_size, shuffle=False, collate_fn=collator.collate)\n",
    "\n",
    "test_iterator = torch.utils.data.DataLoader(test_data, batch_size, shuffle=False, collate_fn=collator.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
       "  (lstm): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTM(input_dim, emb_dim, hid_dim, output_dim, n_layers, dropout, pad_idx)\n",
    "\n",
    "glove = torchtext.experimental.vectors.GloVe(name = '6B',\n",
    "                                             dim = emb_dim)\n",
    "# for n,p in model.named_parameters():\n",
    "#     print(f'name:{n}\\nshape:{p.shape}\\n')\n",
    "model.apply(init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding, unk_tokens = get_pretrained_embedding(model.embedding, glove, vocab, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [-0.2925,  0.1087,  0.7920,  ..., -0.3641,  0.1822, -0.4104],\n",
       "        [-0.7250,  0.7545,  0.1637,  ..., -0.0144, -0.1761,  0.3418],\n",
       "        [ 1.1753,  0.0460, -0.3542,  ...,  0.4510,  0.0485, -0.4015]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.data[pad_idx] = torch.zeros(emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "    \n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = ep_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'bilstm.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('bilstm.pt', map_location=device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4891466498374939"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'I fucking hate you'\n",
    "predict(tokenizer, vocab, model, device, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bentodeploy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bentodeploy.py\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.adapters import JsonInput, JsonOutput\n",
    "from bentoml.artifact import PickleArtifact\n",
    "from bentoml.frameworks.pytorch import PytorchModelArtifact\n",
    "import torchtext\n",
    "import torch\n",
    "@env(infer_pip_packages=True)\n",
    "@artifacts([PytorchModelArtifact('profanity_model'), PickleArtifact('tokenizer'), PickleArtifact('vocab'), PickleArtifact('device')])\n",
    "class ProfanityClassifier(BentoService):    \n",
    "    def predict_actual(self,sentence):\n",
    "        self.artifacts.profanity_model.eval()\n",
    "        device = self.artifacts.device\n",
    "        tokens = self.artifacts.tokenizer.tokenize(sentence)\n",
    "        length = torch.LongTensor([len(tokens)]).to(device)\n",
    "        idx = [self.artifacts.vocab.stoi[token] for token in tokens]\n",
    "        tensor = torch.LongTensor(idx).unsqueeze(-1).to(device)\n",
    "        prediction = self.artifacts.profanity_model(tensor, length)\n",
    "        probabilities = torch.nn.functional.softmax(prediction, dim=-1)\n",
    "        return probabilities.squeeze()[-1].item()\n",
    "    @api(input=JsonInput(), output=JsonOutput())\n",
    "    def predict(self, input):\n",
    "        # result = self.artifacts.profanity_model.predict(sent)\n",
    "        return self.predict_actual(input[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-09-27 23:27:38,180] WARNING - Importing from \"bentoml.artifact.*\" has been deprecated. Instead, use`bentoml.frameworks.*` and `bentoml.service.*`. e.g.:, `from bentoml.frameworks.sklearn import SklearnModelArtifact`, `from bentoml.service.artifacts import BentoServiceArtifact`, `from bentoml.service.artifacts.common import PickleArtifact`\n",
      "[2020-09-27 23:27:39,278] WARNING - BentoML by default does not include spacy and torchvision package when using PytorchModelArtifact. To make sure BentoML bundle those packages if they are required for your model, either import those packages in BentoService definition file or manually add them via `@env(pip_packages=['torchvision'])` when defining a BentoService\n",
      "[2020-09-27 23:27:44,388] INFO - BentoService bundle 'ProfanityClassifier:20200927232742_9097E6' saved to: /home/kishoreganesh/bentoml/repository/ProfanityClassifier/20200927232742_9097E6\n"
     ]
    }
   ],
   "source": [
    "    from bentodeploy import ProfanityClassifier\n",
    "    bento_service = ProfanityClassifier()\n",
    "    bento_service.pack('profanity_model', model)\n",
    "    bento_service.pack('tokenizer', tokenizer)\n",
    "    bento_service.pack('vocab', vocab)\n",
    "    bento_service.pack('device', device)\n",
    "    saved_path = bento_service.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
